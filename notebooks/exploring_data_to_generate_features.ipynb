{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using vectorization to generate features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy\n",
    "import umap\n",
    "import numpy as np \n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10, Category10\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ml_editor.data_processing import format_raw_df, get_split_by_author, get_normalized_series, add_text_features_to_df\n",
    "from ml_editor.data_visualization import plot_embeddings\n",
    "\n",
    "\n",
    "df=pd.read_csv(Path(\"D:\\Project 1\\data\\writers.csv\"))\n",
    "df=format_raw_df(df.copy())\n",
    "\n",
    "train_author, test_author=get_split_by_author(df[df[\"is_question\"]])\n",
    "\n",
    "questions=train_author[train_author[\"is_question\"]]\n",
    "raw_text=questions[\"body_text\"]\n",
    "sent_labels=questions[\"AcceptedAnswerId\"].notna()\n",
    "\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "\n",
    "spacy_emb=train_author[train_author[\"is_question\"]][\"body_text\"].apply(lambda x: nlp(x).vector)\n",
    "embeddings=np.vstack(spacy_emb)\n",
    "\n",
    "umap_embedder=umap.UMAP()\n",
    "umap_emb=umap_embedder.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "def get_interactive_umap_embeddings_plot(umap_vectors, labels, text, legends, tooltip_label=None):\n",
    "    if not tooltip_label:\n",
    "        print(\"Using standard label\")\n",
    "        tooltip_label = labels\n",
    "    w2v_df = pd.DataFrame(umap_vectors, columns=('x', 'y'))\n",
    "    print(len(w2v_df))\n",
    "    w2v_df['label'] = [str(x) for x in labels]\n",
    "    w2v_df['tooltip_label'] = [str(x) for x in tooltip_label]\n",
    "    w2v_df['text'] = list(text)\n",
    "    w2v_df['legends'] = [\"Answered\" if el else \"Unanswered\" for el in list(legends)]\n",
    "    datasource = ColumnDataSource(w2v_df)\n",
    "\n",
    "    color_mapping = CategoricalColorMapper(factors=['True','False'], palette=['#1f77b4', '#ff7f0e'])\n",
    "\n",
    "    TOOLTIPS = [\n",
    "        (\"text\", \"@text\"),\n",
    "        ('got_answer', '@tooltip_label')\n",
    "    ]\n",
    "    hover = HoverTool(tooltips=TOOLTIPS)\n",
    "    hover.attachment ='right'\n",
    "\n",
    "    plot_figure = figure(\n",
    "        title='UMAP projection of questions',\n",
    "        plot_width=900,\n",
    "        plot_height=600,\n",
    "        tools=('pan, wheel_zoom, reset', 'box_zoom', 'undo')\n",
    "    )\n",
    "    plot_figure.add_tools(hover)\n",
    "    \n",
    "    plot_figure.circle(\n",
    "        'x',\n",
    "        'y',\n",
    "        source=datasource,\n",
    "        color=dict(field='label', transform=color_mapping),\n",
    "        legend='legends',\n",
    "        line_alpha=0,\n",
    "        fill_alpha=0.4,\n",
    "        size=5\n",
    "    )\n",
    "    return plot_figure\n",
    "\n",
    "plot_figure = get_interactive_umap_embeddings_plot(umap_emb, sent_labels, raw_text, legends=sent_labels)\n",
    "show(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fill empty rows to allow search to still perform\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody_text_question\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_question_features_containing\u001b[39m(text):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody_text_question\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(text)][[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommentCount\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody_text_question\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore_question\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcceptedAnswerId_question\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Fill empty rows to allow search to still perform\n",
    "df[\"body_text_question\"].fillna(\"\", inplace=True)\n",
    "\n",
    "def show_question_features_containing(text):\n",
    "    return df[df[\"body_text_question\"].str.contains(text)][[\"body_text\", \"CommentCount\",\n",
    "                                                            \"body_text_question\",\n",
    "                                                            \"Score_question\", \"AcceptedAnswerId_question\"]]\n",
    "\n",
    "# Good example of two similar questions\n",
    "show_question_features_containing(\"I'm an amateur writer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Potential features\n",
    "#### Looking through the embeddings and associated rows of data above, it seemed like a few features were predictive of the target class of a question. The ones I idntified were:\n",
    "\n",
    "#### question length: very short questions tended to not get answers\n",
    "#### presence of question mark: the absence of a question mark seemed to lower the chance of an answer\n",
    "#### vocabulary associated with a clear question (action verbs, etc...): unansweredquestions seemed to be missing those\n",
    "#### Did you identify any others? If so, feel free to add them as well.\n",
    "\n",
    "#### We start by creating a feature for the presence of question marks and action verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action_verb\"] = (df[\"body_text\"].str.contains(\"can\", regex=False) | df[\"body_text\"].str.contains(\"What\", regex=False) | df[\"body_text\"].str.contains(\"should\", regex=False))\n",
    "df[\"question_mark\"] = df[\"body_text\"].str.contains(\"?\", regex=False)\n",
    "df[\"text_len\"] = df[\"body_text\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action_verb\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"question_mark\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"norm_text_len\"]= get_normalized_series(df, \"text_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_author, test_author = get_split_by_author(df[df[\"is_question\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_features = np.append(np.array(embeddings), train_author[train_author[\"is_question\"]][[\"action_verb\",\"question_mark\", \n",
    "                                                                            \"norm_text_len\"]], 1)\n",
    "vectorized_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embedder = umap.UMAP()\n",
    "umap_features = umap_embedder.fit_transform(vectorized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_embeddings(umap_features, sent_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_figure = get_interactive_umap_embeddings_plot(umap_features, sent_labels, raw_text, legends=sent_labels)\n",
    "show(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"language_question\"] = (df[\"body_text\"].str.contains(\"punctuate\", regex=False) | df[\"body_text\"].str.contains(\"capitalize\", regex=False) | df[\"body_text\"].str.contains(\"abbreviate\", regex=False)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"body_text\"].str.contains(\"Specifically, how to describe\", regex=False)][[\"body_text\", \"Title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"full_text\"] = df[\"Title\"].str.cat(df[\"body_text\"], sep=' ', na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_text_features_to_df(df.loc[df[\"is_question\"]].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_author, test_author = get_split_by_author(df[df[\"is_question\"]])\n",
    "train_labels  = train_author[\"AcceptedAnswerId\"].notna()\n",
    "\n",
    "train_author[\"vectors\"] = train_author[\"full_text\"].apply(lambda x: nlp(x).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_features = np.append(\n",
    "        np.vstack(train_author[\"vectors\"]),\n",
    "        train_author[\n",
    "            [\n",
    "                \"action_verb_full\",\n",
    "                \"question_mark_full\",\n",
    "                \"norm_text_len\",\n",
    "                \"language_question\",\n",
    "            ]\n",
    "        ],\n",
    "        1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "umap_embedder = umap.UMAP()\n",
    "umap_features = umap_embedder.fit_transform(vectorized_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(umap_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure = get_interactive_umap_embeddings_plot(umap_features, sent_labels, raw_text, legends=sent_labels)\n",
    "show(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
